{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generacion de tf-idf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeFE2KbH+3zVGdrJmljmgi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValdazoAmerico/generacion-tf-idf/blob/main/generacion_de_tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GseDADR2H4nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455c60da-eccb-4665-ce36-68d69522af00"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9936MP8sH95k"
      },
      "source": [
        "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "for para in article_paragraphs:\n",
        "  article_text += para.text\n",
        "\n",
        "corpus = nltk.sent_tokenize(article_text)\n",
        "\n",
        "for i in range(len(corpus)):\n",
        "  corpus[i] = corpus[i].lower()\n",
        "  corpus[i] = re.sub(r'\\W', ' ', corpus[i])\n",
        "  corpus[i] = re.sub(r'\\s+', ' ',corpus[i])\n",
        "\n",
        "wordfreq = {}\n",
        "for sentence in corpus:\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  for token in tokens:\n",
        "    if token not in wordfreq.keys():\n",
        "      wordfreq[token] = 1\n",
        "    else:\n",
        "      wordfreq[token] += 1\n",
        "\n",
        "\n",
        "import heapq\n",
        "most_freq = heapq.nlargest(200,wordfreq,key=wordfreq.get)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qht29PsKvi9Q",
        "outputId": "bb9e27fc-ebb4-4ad3-d15f-0c78b2677f40"
      },
      "source": [
        "most_freq"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['of',\n",
              " 'the',\n",
              " 'and',\n",
              " 'to',\n",
              " 'in',\n",
              " 'language',\n",
              " 'a',\n",
              " 'natural',\n",
              " 'is',\n",
              " 'processing',\n",
              " 'nlp',\n",
              " 'as',\n",
              " 'that',\n",
              " 'cognitive',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'for',\n",
              " 'statistical',\n",
              " 'with',\n",
              " 'tasks',\n",
              " 'on',\n",
              " 'such',\n",
              " 'linguistics',\n",
              " 'rules',\n",
              " 'e',\n",
              " 'neural',\n",
              " 'have',\n",
              " 'models',\n",
              " 'g',\n",
              " 'more',\n",
              " 'are',\n",
              " 'based',\n",
              " 'many',\n",
              " 'has',\n",
              " 'by',\n",
              " 'systems',\n",
              " 'algorithms',\n",
              " 'been',\n",
              " 'input',\n",
              " 'data',\n",
              " 'can',\n",
              " 'speech',\n",
              " 'an',\n",
              " 'which',\n",
              " 'however',\n",
              " 'methods',\n",
              " 'be',\n",
              " 'research',\n",
              " 'real',\n",
              " 'they',\n",
              " 'computer',\n",
              " 'intelligence',\n",
              " 'understanding',\n",
              " 'documents',\n",
              " 'from',\n",
              " 'given',\n",
              " 'or',\n",
              " 'hand',\n",
              " 'grammar',\n",
              " 'part',\n",
              " 'results',\n",
              " 'increasingly',\n",
              " 'used',\n",
              " 'when',\n",
              " 'since',\n",
              " 'some',\n",
              " 'science',\n",
              " 'large',\n",
              " 'task',\n",
              " 'symbolic',\n",
              " '1980s',\n",
              " 'most',\n",
              " 'was',\n",
              " 'this',\n",
              " 'computational',\n",
              " 'deep',\n",
              " 'network',\n",
              " 'set',\n",
              " 'through',\n",
              " 'world',\n",
              " 'valued',\n",
              " 'networks',\n",
              " 'larger',\n",
              " 'use',\n",
              " 'approaches',\n",
              " 'translation',\n",
              " 'trends',\n",
              " 'aspects',\n",
              " 'artificial',\n",
              " 'computers',\n",
              " 'human',\n",
              " 'how',\n",
              " 'process',\n",
              " 'analyze',\n",
              " 'then',\n",
              " 'well',\n",
              " 'frequently',\n",
              " 'involve',\n",
              " 'recognition',\n",
              " 'generation',\n",
              " 'its',\n",
              " 'turing',\n",
              " 'proposed',\n",
              " 'now',\n",
              " 'called',\n",
              " 'but',\n",
              " 'separate',\n",
              " 's',\n",
              " 'chinese',\n",
              " 'answers',\n",
              " 'it',\n",
              " 'up',\n",
              " 'were',\n",
              " 'complex',\n",
              " 'written',\n",
              " 'late',\n",
              " 'revolution',\n",
              " 'due',\n",
              " 'both',\n",
              " 'increase',\n",
              " 'see',\n",
              " 'corpus',\n",
              " 'approach',\n",
              " 'techniques',\n",
              " 'achieve',\n",
              " 'example',\n",
              " 'modeling',\n",
              " 'parsing',\n",
              " 'others',\n",
              " 'study',\n",
              " 'designed',\n",
              " 'produced',\n",
              " '1990s',\n",
              " 'paradigm',\n",
              " 'instead',\n",
              " 'learn',\n",
              " 'examples',\n",
              " 'different',\n",
              " 'these',\n",
              " 'features',\n",
              " 'focused',\n",
              " 'make',\n",
              " 'soft',\n",
              " 'probabilistic',\n",
              " 'decisions',\n",
              " 'attaching',\n",
              " 'weights',\n",
              " 'feature',\n",
              " 'embeddings',\n",
              " 'possible',\n",
              " 'one',\n",
              " 'reliable',\n",
              " 'system',\n",
              " 'tagging',\n",
              " 'especially',\n",
              " 'into',\n",
              " 'subtasks',\n",
              " 'turn',\n",
              " 'largely',\n",
              " 'major',\n",
              " 'field',\n",
              " 'word',\n",
              " 'end',\n",
              " 'higher',\n",
              " 'level',\n",
              " 'intermediate',\n",
              " 'sequence',\n",
              " 'technical',\n",
              " 'build',\n",
              " 'commonly',\n",
              " 'applications',\n",
              " 'long',\n",
              " 'standing',\n",
              " 'among',\n",
              " 'conll',\n",
              " 'shared',\n",
              " 'behaviour',\n",
              " 'knowledge',\n",
              " 'interdisciplinary',\n",
              " 'during',\n",
              " 'ties',\n",
              " 'frameworks',\n",
              " 'ideas',\n",
              " 'subfield',\n",
              " 'concerned',\n",
              " 'interactions',\n",
              " 'between',\n",
              " 'particular',\n",
              " 'program',\n",
              " 'amounts',\n",
              " 'goal',\n",
              " 'capable',\n",
              " 'contents',\n",
              " 'including',\n",
              " 'contextual',\n",
              " 'nuances',\n",
              " 'within',\n",
              " 'them',\n",
              " 'technology',\n",
              " 'accurately']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a374NzITwIjP"
      },
      "source": [
        "word_idf_values = {}\n",
        "for token in most_freq:\n",
        "  doc_containing_word = 0\n",
        "  for document in corpus:\n",
        "    if token in nltk.word_tokenize(document):\n",
        "      doc_containing_word += 1\n",
        "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SQYLSHS_aET"
      },
      "source": [
        "word_tf_values = {}\n",
        "for token in most_freq:\n",
        "  sent_tf_vector = []\n",
        "  for document in corpus:\n",
        "    doc_freq = 0\n",
        "    for word in nltk.word_tokenize(document):\n",
        "      if token == word:\n",
        "        doc_freq += 1\n",
        "    word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
        "    sent_tf_vector.append(word_tf)\n",
        "    \n",
        "  word_tf_values[token] = sent_tf_vector"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxxDixofAQrl",
        "outputId": "2d0ca4d3-bddc-4561-82ca-840cab782ec0"
      },
      "source": [
        "word_tf_values[token]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.045454545454545456,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUUB2jaNA0Ql"
      },
      "source": [
        "tfidf_values = []\n",
        "for token in word_tf_values.keys():\n",
        "  tfidf_sentences = []\n",
        "  for tf_sentence in word_tf_values[token]:\n",
        "    tf_idf_score = tf_sentence * word_idf_values[token]\n",
        "    tfidf_sentences.append(tf_idf_score)\n",
        "  tfidf_values.append(tfidf_sentences)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWaPwtwkBmJC"
      },
      "source": [
        "tf_idf_model = np.asarray(tfidf_values)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk_1hP78BnSM",
        "outputId": "d33f2be7-663f-47f7-f37c-d2b8d79e6ead"
      },
      "source": [
        "print(tf_idf_model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01874153 0.05220854 0.         ... 0.03177911 0.02149763 0.        ]\n",
            " [0.00937076 0.06961139 0.04983542 ... 0.01588956 0.         0.        ]\n",
            " [0.0615983  0.         0.07279799 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.15231777 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.14539423 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.14539423 ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}